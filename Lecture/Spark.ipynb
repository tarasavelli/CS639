{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a5f7c098-8be8-4766-aaeb-526406b2684c",
   "metadata": {},
   "source": [
    "DockerFile\n",
    "FROM ubuntu:22.04\n",
    "RUN apt-get update; apt-get install -y wget curl openjdk-8-jdk python3-pip net-tools lsof nano\n",
    "RUN pip3 install jupyterlab==3.4.5 MarkupSafe==2.0.1 cassandra-driver pyspark==3.2.2 pandas matplotlib\n",
    "\n",
    "# HDFS\n",
    "RUN wget https://pages.cs.wisc.edu/~harter/cs639/hadoop-3.2.4.tar.gz; tar -xf hadoop-3.2.4.tar.gz; rm hadoop-3.2.4.tar.gz\n",
    "\n",
    "# SPARK\n",
    "RUN wget https://pages.cs.wisc.edu/~harter/cs639/spark-3.2.2-bin-hadoop3.2.tgz; tar -xf spark-3.2.2-bin-hadoop3.2.tgz; rm spark-3.2.2-bin-hadoop3.2.tgz\n",
    "\n",
    "ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "ENV PATH=\"${PATH}:/hadoop-3.2.4/bin:/spark-3.2.2-bin-hadoop3.2/bin\"\n",
    "\n",
    "CMD [\"sh\", \"/start.sh\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc24b3-156e-4446-9d33-59b3d767fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"cs544\").getOrCreate())\n",
    "\n",
    "! wget https://pages.cs.wisc.edu/~harter/cs639/data/ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dc485-8838-4698-a92b-1e7984e16bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "nums = list(range(0, 1000000))\n",
    "\n",
    "rdd = sc.parallelize(nums, partitions = 10) ## inside this method we specify partitions\n",
    "\n",
    "inverses = rdd.map(FUNCTION) # using a transformer here\n",
    "\n",
    "def inv(x):\n",
    "    return 1/x\n",
    "\n",
    "inverses = rdd.filter(lambda x: x != 0).map(lambda: x: 1/x)\n",
    "inverses.mean() # this is an action, will trigger all computation done by map()\n",
    "\n",
    "inverses.take(10) # will give the first 10 outputs (first 10 inverse)\n",
    "\n",
    "## gettiing the number of partitions\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da520a8-d943-4f2a-9af6-4c1d8f710306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching\n",
    "\n",
    "rdd.sample(withReplacement = True, fraction = .1, seed = 544).repartition(1).cache(\n",
    ")\n",
    "\n",
    "## remember we can repartition to increase speed of transfoormation and actions\n",
    "\n",
    "# will take a sample of the data\n",
    "import time\n",
    "t0 = time.time()\n",
    "print(sample.mean())\n",
    "t1 = time.time()\n",
    "print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72fa74-d99c-416f-803f-3ead20e1fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://pages.cs.wisc.edu/~harter/cs639/data/ghcnd-stations.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5b713-eb06-48c4-bce4-4aad7fe035b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "df.dtypes\n",
    "## build on rrdd\n",
    "df.rdd # will give the resiliant data. set\n",
    "\n",
    "pandas_df = df.limit(10).toPandas()\n",
    "pandas_df[\"new column\"] = column # can do this in pandas\n",
    "\n",
    "# with spark rdds, cannot mutate but we can create a new df and add columns and rows. \n",
    "\n",
    "newDF = df.withColumn(\"station\", expr(\"substring(value, 0, 11)\")) # will create a new dataframe with specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561df0e-52b1-4ad1-b504-dde62444aecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
